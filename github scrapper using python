from selenium import webdriver
from selenium.webdriver.common.by import By
import time
from selenium.webdriver.chrome.service import Service

scrape = input("What page would you like to scrape? ")
cdp = "/home/dehant/Desktop/cw/chromedriver-linux64/chromedriver"

service = Service(executable_path=cdp)
driver = webdriver.Chrome(service=service)
driver.get(f"{scrape}")
print(f"Scraping page: {scrape}")

# Corrected URL for the repository page
repo = "https://github.com/usernam121"  # Change to the correct repository link

# Ensure we wait for the page to load
time.sleep(2)

# Find repositories (use correct method based on actual page structure)
res = driver.find_elements(By.CLASS_NAME, "repo")
print("REPOS FOUND:", len(res))

# Initialize links
links = []
flink = []


def going_for_raw(second_page):
    print(f"TRYING TO LOAD RAW PAGE: {second_page}")
    driver.get(second_page)
    try:
        # Look for the raw file link and click it (adjusting for the correct XPath)
        raw = driver.find_element(By.XPATH, "//a[contains(@href, 'raw')]")
        raw.click()
        time.sleep(2)  # Wait for the raw page to load
        html = driver.page_source
        if "password" in html:
            print(f"Found password --> {second_page}")
        else:
            print(f"No password found at {second_page}")
    except Exception as e:
        print(f"Error while accessing raw page: {e}")


def loop(next_page):
    print(f"Trying to get the next page :{next_page}")
    driver.get(next_page)
    time.sleep(2)  # Let the repository page load properly
    print(f"INSIDE REPO PAGE: {next_page}")

    # Find all links to the files in the repository using the correct class name
    res2 = driver.find_elements(By.CLASS_NAME, "Link--primary")

    # Get the text from elements immediately to avoid stale references
    file_names = [a.text for a in res2 if "py" in a.text.lower()]

    for filename in file_names:
        try:
            second_page = f"{next_page}/blob/main/{filename}"
            print(f"GOING TO FILE: {second_page}")
            going_for_raw(second_page)
        except Exception as e:
            print(f"Error processing file: {e}")


# Gather all the repository links (use the found repository names or links)
for i in res:
    links.append(i.text)

# Now process the gathered links
for l in links:
    next_page = f"{repo}/{l}"
    flink.append(next_page)
    loop(next_page)

# End the session after processing
driver.quit()
